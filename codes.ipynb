{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5704201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service as EdgeService  # Import EdgeService\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd  # for storing data in a structured way (optional)\n",
    "\n",
    "def extract_station_data_from_page(driver):\n",
    "    \"\"\"Extracts station data from the current page of the table.\"\"\"\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    table = soup.find('table', class_='stn-dir-list-tbl')  # Find the table\n",
    "\n",
    "    if table is None:\n",
    "        print(\"Table not found on this page.\")\n",
    "        return []\n",
    "\n",
    "    station_data = []\n",
    "    rows = table.find_all('tr')  # Get all table rows\n",
    "\n",
    "    # Extract header information\n",
    "    header_row = rows[0]\n",
    "    header_cells = header_row.find_all('th')\n",
    "    headers = [cell.text.strip() for cell in header_cells]\n",
    "\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        cells = row.find_all('td')\n",
    "        if not cells:\n",
    "            continue  # Skip empty rows\n",
    "\n",
    "        row_data = {}\n",
    "        for i, cell in enumerate(cells):\n",
    "            if i == 1:  # Handle the station name (which contains a link)\n",
    "                link = cell.find('a')\n",
    "                if link:\n",
    "                    row_data[headers[i]] = link.text.strip()  # Extract link text\n",
    "                else:\n",
    "                    row_data[headers[i]] = cell.text.strip()\n",
    "            else:\n",
    "                row_data[headers[i]] = cell.text.strip()\n",
    "\n",
    "        station_data.append(row_data)\n",
    "\n",
    "    return station_data\n",
    "\n",
    "def extract_all_pages_data(driver, letter):\n",
    "    \"\"\"Extracts station data from all pages for a given letter.\"\"\"\n",
    "    all_data = []\n",
    "    page_num = 1  # Start at page 1\n",
    "\n",
    "    while True:\n",
    "        print(f\"Extracting data from page {page_num} for letter '{letter}'\")\n",
    "        page_data = extract_station_data_from_page(driver)  #Extract data from current page\n",
    "        if not page_data:\n",
    "            print(f\"No data found on page {page_num} for letter '{letter}'. Stopping.\")\n",
    "            break\n",
    "\n",
    "        all_data.extend(page_data)\n",
    "\n",
    "        # Find the \"Next\" button and click it. Use a try-except block in case the button is disabled or not found\n",
    "        try:\n",
    "            next_button = driver.find_element(By.LINK_TEXT, \"Next â†’\")\n",
    "            if 'disabled' in next_button.get_attribute('class'):\n",
    "                print(\"Next button is disabled, no more pages\")\n",
    "                break # Exit if no next page\n",
    "            next_button.click()\n",
    "            driver.implicitly_wait(5) #wait for next page to load\n",
    "            page_num += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Next button not found or could not be clicked. Assuming last page reached. {e}\")\n",
    "            break\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# def extract_station_data_for_letter(url, letter):\n",
    "#     \"\"\"Extracts all station data for a given letter, handling multiple pages.\"\"\"\n",
    "\n",
    "#     driver = None  # Initialize driver outside the try block\n",
    "#     try:\n",
    "#         # 1. Set up Selenium Edge WebDriver\n",
    "#         driver = webdriver.Edge()\n",
    "#         driver.get(url)\n",
    "\n",
    "#         # 2. Click on the specified letter\n",
    "#         letter_element = driver.find_element(By.XPATH, f'//a[@class=\"StationName\" and @data-station-name=\"{letter}\"]')\n",
    "#         letter_element.click()\n",
    "#         driver.implicitly_wait(5)\n",
    "\n",
    "#         # 3. Extract data from all pages\n",
    "#         all_data = extract_all_pages_data(driver, letter)  # Modified function call\n",
    "#         return all_data\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     finally:\n",
    "#         # 5. Close the WebDriver\n",
    "#         if driver:  # Check if the driver was initialized\n",
    "#             driver.quit()\n",
    "#             # Modify the URL to include the letter and page number directly\n",
    "def extract_station_data_for_letter(url, letter):\n",
    "    \"\"\"Extracts all station data for a given letter, handling multiple pages.\"\"\"\n",
    "\n",
    "    driver = None  # Initialize driver outside the try block\n",
    "    try:\n",
    "        # 1. Set up Selenium Edge WebDriver\n",
    "        driver = webdriver.Edge()\n",
    "\n",
    "        # 2. Extract data from all pages\n",
    "        all_data = []\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            # Construct the URL with the letter and page number\n",
    "            page_url = f\"{url}?name={letter}&page={page_num}\"\n",
    "            print(f\"Accessing URL: {page_url}\")\n",
    "            driver.get(page_url)\n",
    "            driver.implicitly_wait(5)\n",
    "\n",
    "            # Extract data from the current page\n",
    "            page_data = extract_station_data_from_page(driver)\n",
    "            if not page_data:\n",
    "                print(f\"No data found on page {page_num} for letter '{letter}'. Stopping.\")\n",
    "                break\n",
    "\n",
    "            all_data.extend(page_data)\n",
    "            page_num += 1\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "    finally:\n",
    "        # 3. Close the WebDriver\n",
    "        if driver:  # Check if the driver was initialized\n",
    "            driver.quit()\n",
    "# Example Usage:\n",
    "url = \"https://www.railyatri.in/stations\"  # Replace with the actual URL of the webpage\n",
    "letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"  # All letters to process\n",
    "\n",
    "all_data = {} # Store data for all letters\n",
    "\n",
    "for letter in letters:\n",
    "    \n",
    "    \n",
    "    data = extract_station_data_for_letter(url, letter)\n",
    "\n",
    "    if data:\n",
    "        print(f\"Found {len(data)} stations starting with {letter}\")\n",
    "        all_data[letter] = data # Store data for the letter\n",
    "    else:\n",
    "        print(f\"No data found for letter '{letter}'.\")\n",
    "        all_data[letter] = []\n",
    "\n",
    "# Optional: Convert all data to Pandas DataFrame and save to CSV\n",
    "all_dfs = {}\n",
    "for letter, data in all_data.items():\n",
    "    all_dfs[letter] = pd.DataFrame(data)\n",
    "\n",
    "# Combine all dataframes into a single dataframe:\n",
    "combined_df = pd.concat(all_dfs.values(), ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv(\"all_stations.csv\", index=False)\n",
    "print(\"Data saved to all_stations.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768aba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "PAGE_URL = \"https://www.ixigo.com/hotels\"\n",
    "DESTINATION_CITY = \"Lucknow\"\n",
    "\n",
    "# --- WebDriver Setup ---\n",
    "\n",
    "driver = webdriver.Edge()\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"Navigating to {PAGE_URL}...\")\n",
    "    driver.get(PAGE_URL)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # --- 1. Handle Destination Input ---\n",
    "    print(f\"Locating and updating Destination input...\")\n",
    "\n",
    "    # Retry logic for StaleElementReferenceException\n",
    "    for attempt in range(3):  # Try up to 3 times\n",
    "        try:\n",
    "            destination_input_locator = (By.XPATH, \"//p[text()='Destination']/following-sibling::input\")\n",
    "            destination_input = wait.until(EC.element_to_be_clickable(destination_input_locator))\n",
    "\n",
    "            # *CLICK THE INPUT FIELD FIRST*\n",
    "            destination_input.click()\n",
    "            print(\"Clicked on the Destination input field.\")\n",
    "\n",
    "            destination_input.clear()\n",
    "            destination_input.send_keys(DESTINATION_CITY)\n",
    "            print(f\"Entered '{DESTINATION_CITY}' into Destination input.\")\n",
    "            break  # If successful, exit the loop\n",
    "        except StaleElementReferenceException:\n",
    "            print(f\"StaleElementReferenceException occurred (attempt {attempt + 1}). Retrying...\")\n",
    "            time.sleep(1)  # Wait a bit before retrying\n",
    "    else:  # If the loop completes without a 'break'\n",
    "        print(\"Failed to locate and update Destination input after multiple attempts.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Use the provided HTML to locate the correct element:\n",
    "        mumbai_option_locator = (By.XPATH, f\"//div[@data-testid='{DESTINATION_CITY}' and @role='button']\")\n",
    "        mumbai_option = wait.until(EC.element_to_be_clickable(mumbai_option_locator))\n",
    "        mumbai_option.click()\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Timed out waiting for the 'Mumbai' dropdown item to be clickable.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while selecting 'Mumbai' from dropdown: {e}\")\n",
    "        raise\n",
    "\n",
    "    # --- 3. Handle Search Button ---\n",
    "    print(\"Locating Search button...\")\n",
    "    search_button_locator = (By.CSS_SELECTOR, 'button[data-testid=\"search-hotels\"]')\n",
    "    search_button = wait.until(EC.element_to_be_clickable(search_button_locator))\n",
    "    print(\"Clicking Search button...\")\n",
    "    search_button.click()\n",
    "    print(\"Search initiated successfully!\")\n",
    "\n",
    "    time.sleep(5)\n",
    "    \n",
    "    new_url = driver.current_url\n",
    "    print(f\"New URL: {new_url}\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"Error: Timed out waiting for one or more elements to load or become interactive.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aadb77",
   "metadata": {},
   "source": [
    "## Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "edge_driver_path = r\"D:\\msedgedriver.exe\"  # Replace with your path\n",
    "edge_options = EdgeOptions()\n",
    "#edge_options.add_argument(\"--headless=new\")  # Uncomment for headless mode\n",
    "edge_options.add_argument(\"--start-maximized\")\n",
    "edge_options.add_argument(\"--force-device-scale-factor=0.05\")\n",
    "edge_service = EdgeService(executable_path=edge_driver_path)\n",
    "driver = webdriver.Edge(service=edge_service, options=edge_options)\n",
    "\n",
    "\n",
    "def access_ixigo(from_city=\"LKO\", to_city=\" ,\n",
    "                 date=\"17042025\", return_date=\"22042025\",\n",
    "                 adults=2, children=0, infants=0, flight_class=\"e\",\n",
    "                 hbs=True):\n",
    "    \"\"\"\n",
    "    Accesses the Ixigo flight search website and extracts flight information\n",
    "    *including* structured data extracted from the HTML of each flight card.\n",
    "\n",
    "    Args:\n",
    "        from_city (str): Departure city code.\n",
    "        to_city (str): Destination city code.\n",
    "        date (str): Departure date (DDMMYYYY).\n",
    "        return_date (str): Return date (DDMMYYYY).\n",
    "        adults (int): Number of adults.\n",
    "        children (int): Number of children.\n",
    "        infants (int): Number of infants.\n",
    "        flight_class (str): Flight class (e.g., \"e\" for economy).\n",
    "        hbs (bool): Whether to search for hotel-bundled savings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the URL (using f-strings for readability)\n",
    "    base_url = \"https://www.ixigo.com/search/result/flight\"\n",
    "    params = {\n",
    "        \"from\": from_city,\n",
    "        \"to\": to_city,\n",
    "        \"date\": date,\n",
    "        \"returnDate\": return_date,\n",
    "        \"adults\": adults,\n",
    "        \"children\": children,\n",
    "        \"infants\": infants,\n",
    "        \"class\": flight_class,\n",
    "        \"source\": \"Search+Form\",  # Note: The space is encoded as '+'\n",
    "        \"hbs\": str(hbs).lower()  # Convert boolean to lowercase string\n",
    "    }\n",
    "\n",
    "    # Build the query string\n",
    "    query_string = \"&\".join([f\"{key}={quote(str(value))}\" for key, value in params.items()])  # use quote to encode spaces and special characters\n",
    "    url = f\"{base_url}?{query_string}\"\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Allow time for page to load and flights to populate\n",
    "        \n",
    "\n",
    "        print(\"Page Title:\", driver.title)\n",
    "\n",
    "        flight_data = extract_ixigo_flight_data(driver)\n",
    "\n",
    "        # with open(\"ixigo_flights.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        #     json.dump(flight_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(\"Flight data saved to ixigo_flights.json\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    # finally:\n",
    "        #driver.quit()  # Ensure the driver quits, especially after an error\n",
    "\n",
    "\n",
    "def extract_ixigo_flight_data(driver):\n",
    "    \"\"\"\n",
    "    Extracts flight data from the Ixigo search results page,\n",
    "    extracting both structured data and using BeautifulSoup to parse the HTML.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a flight.\n",
    "    \"\"\"\n",
    "    # WebDriverWait(driver, 10).until(\n",
    "    #         EC.presence_of_element_located((By.CSS_SELECTOR, 'div[class=\"shadow-[0px_2px_5px_0px_rgba(0,0,0,0.10)] p-15 mb-20  rounded-10 relative cursor-pointer bg-white border border-white transition-all duration-300 ease-in hover:scale-[1.01] hover:shadow-300 hover:duration-300 hover:ease-out\"]'))\n",
    "    #     )\n",
    "\n",
    "    flight_data = []  # Use deque for efficient appending\n",
    "    flight_cards_data = []\n",
    "    flight_cards = driver.find_elements(By.XPATH, \"//div[contains(@class, 'cursor-pointer')]\")\n",
    "    # for  flight_card in flight_cards:\n",
    "    #     print(flight_card)\n",
    "    print(len(flight_cards))\n",
    "    for card in flight_cards:\n",
    "        try:\n",
    "            # **Scroll to the element before extracting**\n",
    "            # driver.execute_script(\"arguments[0].scrollIntoView();\", card)\n",
    "            time.sleep(0.5)  # Give it a brief pause to fully load after scrolling\n",
    "\n",
    "            # **Extract the HTML**\n",
    "            html_string = card.get_attribute('outerHTML')\n",
    "\n",
    "            # **Parse the HTML with BeautifulSoup**\n",
    "            flight = extract_flight_data_from_html(html_string)  # Use the parsing function\n",
    "\n",
    "            flight_data.append(flight)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data from a flight card: {e}\")\n",
    "            continue\n",
    "\n",
    "    return flight_data\n",
    "\n",
    "\n",
    "def extract_flight_data_from_html(html_string):\n",
    "    \"\"\"\n",
    "    Extracts specific flight data from the HTML of a flight card using BeautifulSoup.\n",
    "    This function extracts:\n",
    "    - Fastest flight indicator\n",
    "    - Airline\n",
    "    - Departure Time\n",
    "    - Arrival Time\n",
    "    - Flight Duration\n",
    "    - Stops\n",
    "    - Price\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "\n",
    "    flight = {}\n",
    "\n",
    "    # Fastest flight indicator\n",
    "    fastest_element = soup.find('div', class_='text-selection-outline border-selection-outline min-h-20 icon-sm body-xs inline-flex items-center font-normal rounded-full px-px border border-solid bg-white')\n",
    "    flight['fastest'] = True if fastest_element else False\n",
    "\n",
    "    # Airline\n",
    "    airline_element = soup.find('p', class_='body-sm body-sm')\n",
    "    flight['airline'] = airline_element.text.strip() if airline_element else None\n",
    "\n",
    "    # Departure Time\n",
    "    departure_time_element = soup.find('h5', class_='h5 text-primary font-medium')\n",
    "    flight['departure_time'] = departure_time_element.text.strip() if departure_time_element else None\n",
    "\n",
    "    # Arrival Time (look for the element with GOX as destination which is now generic GOI)\n",
    "    arrival_time_element = soup.find_all('h5', class_='h5 text-primary font-medium')[1]\n",
    "    flight['arrival_time'] = arrival_time_element.text.strip() if len(soup.find_all('h5', class_='h5 text-primary font-medium')) > 1 and arrival_time_element else None #get the second element\n",
    "\n",
    "    # Flight Duration\n",
    "    duration_element = soup.find('p', class_='body-sm text-secondary')\n",
    "    flight['duration'] = duration_element.text.strip() if duration_element else None\n",
    "\n",
    "    # Stops\n",
    "    stops_element = soup.find_all('p', class_='body-sm text-secondary')\n",
    "    flight['stops'] = stops_element[1].text.strip() if len(stops_element) > 1 and stops_element[1] else None #get the second element\n",
    "\n",
    "\n",
    "    # Price\n",
    "    price_element = soup.find('h5', {'data-testid': 'pricing'})\n",
    "    flight['price'] = price_element.text.strip() if price_element else None\n",
    "\n",
    "    return flight\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    access_ixigo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
